{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puyopuyo RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- action\n",
    "    - 操作 0: Left, 1: Right, 2: A, 3: B, 4: Bottom\n",
    "- state\n",
    "    - puyoの配列\n",
    "    - 0: red, 1: blue, 2: yellow, 3: green\n",
    "- NN\n",
    "    - CNN\n",
    "    - Linear\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruihirano/.pyenv/versions/3.7.0/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from gym import wrappers\n",
    "from datetime import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from enum import Enum\n",
    "from typing import NamedTuple, List\n",
    "import pandas as pd\n",
    "import os\n",
    "import uuid\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PuyoType(Enum)\n",
    "    NONE = 0\n",
    "    RED = 1\n",
    "    BLUE = 2\n",
    "    YELLOW = 3\n",
    "    GREEN = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        #print(\"args\", state, action, next_state, reward)\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(state, action, next_state, reward)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Environment\n",
    "\n",
    "class EnvParameter(NamedTuple):\n",
    "    max_lot: int    # 最大数量\n",
    "    spread: int     # スプレッド\n",
    "    window_size: int\n",
    "    \n",
    "        \n",
    "class Environment():\n",
    "    def __init__(self, data, env_param, MONITOR, reward_func):\n",
    "        self.MONITOR = MONITOR\n",
    "        self.reward_func = reward_func\n",
    "        self.param = env_param\n",
    "        self.forex_data = data # set forex_data  [t=0 data, t=1 data,....., t=x data]\n",
    "        self.history = []\n",
    "        self.observ = [[0]*6]*13\n",
    "        self.steps = self.param.window_size\n",
    "        \n",
    "        #if MONITOR:\n",
    "        #    wrappers.Monitor(self.env, \"./tmp\", force=True)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.history = []\n",
    "        return self.get_observe()\n",
    "        \n",
    "    def step(self, action): # action: 0 is None, 1 is Buy, 2 is Sell\n",
    "        \n",
    "        action = ActionType(action)\n",
    "        \n",
    "        if self.is_build(action):\n",
    "            ''' 新規取引 '''\n",
    "            self.position = PositionType.LONG if action == ActionType.BUY else PositionType.SHORT\n",
    "            self.transaction = Transaction(start_date=date, pair=PairType.USD_JPY, lot=lot, position=self.position, entry_rate=close)\n",
    "\n",
    "        elif self.is_release(action):\n",
    "            ''' 決済 '''\n",
    "            # 取引内容を追記\n",
    "            self.transaction.settle(end_date=date, settle_rate=close)\n",
    "            self.history.append(self.transaction)\n",
    "            # エピソード終了\n",
    "            self.done = True\n",
    "            \n",
    "        self.done, chain_num = self.check_chain()\n",
    "        \n",
    "        ''' 報酬計算 '''\n",
    "        reward = calc_reward(chain_num)\n",
    "        \n",
    "        ''' 次のステップの環境を作成 '''\n",
    "        self.steps += 1\n",
    "        next_observe = self.get_observe()\n",
    "\n",
    "        return next_observe, reward, self.done, self.transaction\n",
    "        \n",
    "    def get_action_num(self): # action num is 3\n",
    "        return len(ActionType)\n",
    "    \n",
    "    def get_observ_num(self): # observ_num is window_size\n",
    "        return self.param.window_size\n",
    "    \n",
    "    def get_reward(self, chain_num):\n",
    "        ''' 連鎖の1/10を報酬とする。終了してない場合0.1を加算 '''\n",
    "        if self.done:\n",
    "            return chain_num / 10\n",
    "        else:\n",
    "            return 0.1\n",
    "    \n",
    "    def get_observe(self):\n",
    "        puyo1 = PuyoType(torch.randn(len(PuyoType)))\n",
    "        puyo2 = PuyoType(torch.randn(len(PuyoType)))\n",
    "        self.history.append([puyo1, puyo2])\n",
    "        return self.history\n",
    "    \n",
    "    def check_chain(self):\n",
    "        return 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    def show(self):\n",
    "        return 0\n",
    "        #if self.MONITOR:\n",
    "        #    self.env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        return 0\n",
    "        #if self.MONITOR:\n",
    "        #    self.env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, brain):\n",
    "        '''エージェントが行動を決定するための頭脳を生成'''\n",
    "        self.brain = brain\n",
    "        \n",
    "    def learn(self):\n",
    "        '''Q関数を更新する'''\n",
    "        loss = self.brain.optimize()\n",
    "        return loss\n",
    "        \n",
    "    def modify_goal(self):\n",
    "        '''Target Networkを更新する'''\n",
    "        self.brain.update_target_model()\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state)\n",
    "        return action\n",
    "    \n",
    "    def memorize(self, state, action, next_state, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, next_state, reward)\n",
    "    \n",
    "    def predict_action(self, state):\n",
    "        '''行動を予測する'''\n",
    "        action = self.brain.predict(state)\n",
    "        return action\n",
    "    \n",
    "    def record(self, name):\n",
    "        '''モデルを保存する'''\n",
    "        self.brain.save_model(name)\n",
    "        \n",
    "    def remember(self, name):\n",
    "        '''モデルを読み込む'''\n",
    "        self.brain.read_model(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.profit_durations = [0]\n",
    "        self.total_profit_durations = [0]\n",
    "        self.loss_durations = []\n",
    "        self.TARGET_UPDATE = 10\n",
    "        self.episode = 0\n",
    "        \n",
    "    def train(self, save_name):\n",
    "        while self.env.is_finish() == False:\n",
    "            print(\"episode: \", self.episode)\n",
    "            state = self.env.reset()\n",
    "            for t in count():\n",
    "                ''' 行動を決定する '''\n",
    "                # Select and perform an action\n",
    "                action = self.agent.select_action(state) # input ex: <list> [0, 0, 0, 0], output ex: <int> 0 or 1\n",
    "                print(\"action\", action)\n",
    "                lot = 1\n",
    "                \n",
    "                ''' 行動に対する環境や報酬を取得する '''\n",
    "                next_state, reward, done, transaction = self.env.step(action, lot)  # state [0,0,0,0...window_size], reward 1.0, done False, input: action 0 or 1 or 2\n",
    "                \n",
    "                ''' 終了時はnext_state_valueをNoneとする '''\n",
    "                if done:\n",
    "                    next_state_value = None\n",
    "                else:\n",
    "                    next_state_value = torch.tensor([next_state], device=device, dtype=torch.float32)\n",
    "                #print(\"after action: {0}, state: {1}, next_state: {2}, reward: {3} done: {4}\".format(action, state, next_state, reward, done))\n",
    "                \n",
    "\n",
    "                ''' エージェントに記憶させる '''\n",
    "                # Store the transition in memory\n",
    "                self.agent.memorize(\n",
    "                    torch.tensor([state], device=device, dtype=torch.float32), \n",
    "                    torch.tensor([[action]], device=device), \n",
    "                    next_state_value, \n",
    "                    torch.tensor([reward], device=device)\n",
    "                )\n",
    "                \n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                ''' エージェントに学習させる '''\n",
    "                # Perform one step of the optimization (on the target network)\n",
    "                # update q network\n",
    "                loss = self.agent.learn()\n",
    "                print(\"loss: \", loss)\n",
    "                if loss != None:\n",
    "                    self.loss_durations.append(loss)\n",
    "                \n",
    "                if done:\n",
    "                    ''' 終了時に結果をプロット '''\n",
    "                    print(\"sdate: {0}, edate: {1}, position: {2}, profit: {3}\".format(transaction.start_date, transaction.end_date, transaction.position, transaction.profit))\n",
    "                    total_profit = transaction.profit + self.total_profit_durations[-1]\n",
    "                    self.total_profit_durations.append(total_profit)\n",
    "                    self.profit_durations.append(transaction.profit)\n",
    "                    self.plot_durations()\n",
    "                    self.episode += 1\n",
    "                    break\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            if self.episode % self.TARGET_UPDATE == 0:\n",
    "                ''' 目標を修正する '''\n",
    "                self.agent.modify_goal()\n",
    "\n",
    "        ''' モデルを保存する '''\n",
    "        # モデルの保存\n",
    "        self.agent.record(save_name)\n",
    "        print('Complete')\n",
    "        \n",
    "        \n",
    "    def plot_durations(self):\n",
    "        #figure()でグラフを表示する領域をつくり，figというオブジェクトにする．\n",
    "        fig = plt.figure()\n",
    "\n",
    "        #add_subplot()でグラフを描画する領域を追加する．引数は行，列，場所\n",
    "        ax1 = fig.add_subplot(2, 2, 1)\n",
    "        ax2 = fig.add_subplot(2, 2, 2)\n",
    "        ax3 = fig.add_subplot(2, 2, 3)\n",
    "\n",
    "        x1 = [s for s in range(len(self.profit_durations))]\n",
    "        y1 = self.profit_durations\n",
    "        x2 = [s for s in range(len(self.total_profit_durations))]\n",
    "        y2 = self.total_profit_durations\n",
    "        x3 = [s for s in range(len(self.loss_durations))]\n",
    "        y3 = self.loss_durations\n",
    "\n",
    "        c1,c2,c3 = \"blue\",\"green\", \"red\"      # 各プロットの色\n",
    "        l1,l2,l3 = \"profit\",\"total\", \"loss\"   # 各ラベル\n",
    "\n",
    "        ax1.plot(x1, y1, color=c1, label=l1)\n",
    "        ax2.plot(x2, y2, color=c2, label=l2)\n",
    "        ax3.plot(x3, y3, color=c3, label=l3)\n",
    "        ax1.legend(loc = 'upper right') #凡例\n",
    "        ax2.legend(loc = 'upper right') #凡例\n",
    "        ax3.legend(loc = 'upper right') #凡例\n",
    "        fig.tight_layout()              #レイアウトの設定\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Examiner():\n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.profit_durations = [0]\n",
    "        self.total_profit_durations = [0]\n",
    "        self.episode = 0\n",
    "        self.stats = Statistics()\n",
    "        self.transactions = []\n",
    "        \n",
    "    def evaluate(self, file_name):\n",
    "        self.agent.remember(file_name)\n",
    "        \n",
    "        while self.env.is_finish() == False:\n",
    "            print(\"episode: \", self.episode)\n",
    "            state = self.env.reset()\n",
    "            for t in count():\n",
    "                #self.env.show()\n",
    "                \n",
    "                ''' 行動を決定する '''\n",
    "                action = self.agent.predict_action(state) # input ex: <list> [0, 0, 0, 0], output ex: <int> 0 or 1\n",
    "                print(\"action: \", action)\n",
    "                lot = 1\n",
    "                \n",
    "                ''' 行動に対する環境や報酬を取得する '''\n",
    "                next_state, reward, done, transaction = self.env.step(action, lot)  # state [0,0,0,0...window_size], reward 1.0, done False, input: action 0 or 1 or 2\n",
    "                \n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    ''' 終了時に結果をプロット '''\n",
    "                    print(\"step: \", t)\n",
    "                    print(\"sdate: {0}, edate: {1}, position: {2}, profit: {3}\".format(transaction.start_date, transaction.end_date, transaction.position, transaction.profit))\n",
    "                    total_profit = transaction.profit + self.total_profit_durations[-1]\n",
    "                    self.total_profit_durations.append(total_profit)\n",
    "                    self.profit_durations.append(transaction.profit)\n",
    "                    self.plot_durations()\n",
    "                    self.transactions.append(transaction)\n",
    "                    self.episode += 1\n",
    "                    break\n",
    "                    \n",
    "        #self.env.close()\n",
    "        ''' 統計計算 '''\n",
    "        self.stats.run(self.transactions)\n",
    "        self.stats.show()\n",
    "        print('Complete')\n",
    "                    \n",
    "    def plot_durations(self):\n",
    "        #figure()でグラフを表示する領域をつくり，figというオブジェクトにする．\n",
    "        fig = plt.figure()\n",
    "\n",
    "        #add_subplot()でグラフを描画する領域を追加する．引数は行，列，場所\n",
    "        ax1 = fig.add_subplot(1, 2, 1)\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "        x1 = [s for s in range(len(self.profit_durations))]\n",
    "        y1 = self.profit_durations\n",
    "        x2 = [s for s in range(len(self.total_profit_durations))]\n",
    "        y2 = self.total_profit_durations\n",
    "\n",
    "        c1,c2 = \"blue\",\"green\"      # 各プロットの色\n",
    "        l1,l2 = \"profit\",\"total\"   # 各ラベル\n",
    "\n",
    "        ax1.plot(x1, y1, color=c1, label=l1)\n",
    "        ax2.plot(x2, y2, color=c2, label=l2)\n",
    "        ax1.legend(loc = 'upper right') #凡例\n",
    "        ax2.legend(loc = 'upper right') #凡例\n",
    "        fig.tight_layout()              #レイアウトの設定\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs_num, hidden_size, outputs_num):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs_num, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, outputs_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        y = F.relu(self.fc4(h))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainParameter(NamedTuple):\n",
    "    batch_size: int\n",
    "    gamma : float\n",
    "    eps_start : float\n",
    "    eps_end: float\n",
    "    eps_decay: int\n",
    "    capacity: int\n",
    "    hidden_size: int\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self, param, num_observ, num_actions):\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        # Brain Parameter\n",
    "        self.BATCH_SIZE = param.batch_size\n",
    "        self.GAMMA = param.gamma\n",
    "        self.EPS_START = param.eps_start\n",
    "        self.EPS_END = param.eps_end\n",
    "        self.EPS_DECAY = param.eps_decay\n",
    "        self.CAPACITY = param.capacity\n",
    "        self.HIDDEN_SIZE = param.hidden_size\n",
    "        \n",
    "        # 経験を保存するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(self.CAPACITY)\n",
    "        \n",
    "        #print(self.model) # ネットワークの形を出力\n",
    "        self.num_observ = num_observ\n",
    "        #print(self.num_observ)\n",
    "        self.num_actions = num_actions # 行動の数を取得\n",
    "        self.policy_net = DQN(self.num_observ, self.HIDDEN_SIZE, self.num_actions).to(device)\n",
    "        self.target_net = DQN(self.num_observ, self.HIDDEN_SIZE, self.num_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        \n",
    "    def optimize(self):\n",
    "        if len(self.memory) < self.BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # 訓練モード\n",
    "        self.policy_net.train()\n",
    "        \n",
    "        ''' batch化する '''\n",
    "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        state_batch = torch.cat(batch.state) # state: tensor([[0.5, 0.4, 0.5, 0], ...]) size(32, 4)\n",
    "        action_batch = torch.cat(batch.action) # action: tensor([[1],[0],[0]...]) size(32, 1) \n",
    "        reward_batch = torch.cat(batch.reward) # reward: tensor([1, 1, 1, 0, ...]) size(32)\n",
    "        #print(\"state_batch: \", state_batch, state_batch.size())\n",
    "        #print(\"action_batch: \", action_batch, action_batch.size())\n",
    "        #print(\"reward_batch: \", reward_batch, reward_batch.size())\n",
    "\n",
    "\n",
    "        ''' 出力データ：行動価値を作成 '''\n",
    "        # 出力actionの値のうちaction_batchが選んだ方を抽出（.gather()）\n",
    "        # action_batch = [[0], [1], [1]...] action_value = [[0.01, 0.03], [0.03, 0], [0, 0.02]...]\n",
    "        # state_action_values = [[0.01], [0], [0.02]]\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch) # size(32, 1)\n",
    "        #print(\"state_action_values2\", self.policy_net(state_batch), self.policy_net(state_batch).size())\n",
    "        #print(\"state_action_values\", state_action_values, state_action_values.size())\n",
    "\n",
    "        ''' 教師データを作成する '''\n",
    "        ''' target = 次のステップでの行動価値の最大値 * 時間割引率 + 即時報酬 '''\n",
    "         # doneされたかどうか doneであればfalse\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), device=device, dtype=torch.bool)\n",
    "        #print(\"non_final_mask: \", non_final_mask, non_final_mask.size())\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        #print(\"non_final_next_state: \", non_final_next_states, non_final_next_states.size())\n",
    "        \n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "        \n",
    "        # 大きい方を選択して一次元にする\n",
    "        # done時は0\n",
    "        # target_net: [[0, 0.1], [2, 0.2]...], size(32, 2)      next_state_values: [0.1, 2...], size(32)\n",
    "        # 次の環境での行動価値\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach() # size(32)\n",
    "\n",
    "        # target = 次のステップでの行動価値の最大値 * 時間割引率 + 即時報酬\n",
    "        expected_state_action_values = ((next_state_values * self.GAMMA) + reward_batch).unsqueeze(1) # size(32, 1)\n",
    "        #print(\"expected_state_value: \", expected_state_action_values, expected_state_action_values.size())\n",
    "\n",
    "        ''' Loss を計算'''\n",
    "        # Compute Huber loss\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "\n",
    "        ''' 勾配計算、更新 '''\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # モデルの重みをtarget_networkにコピー\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def decide_action(self, state):\n",
    "        state = torch.tensor(state, device=device).float()\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                action = np.argmax(self.policy_net(state).tolist())\n",
    "                return action\n",
    "        else:\n",
    "            return random.randrange(self.num_actions)\n",
    "    \n",
    "    \n",
    "    def save_model(self, name):\n",
    "        torch.save(self.policy_net.state_dict(), name)\n",
    "        \n",
    "    def read_model(self, name):\n",
    "        param = torch.load(name)\n",
    "        self.policy_net.load_state_dict(param)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        state = torch.tensor(state, device=device).float()\n",
    "        self.policy_net.eval() # ネットワークを推論モードに切り替える\n",
    "        with torch.no_grad():\n",
    "            action = np.argmax(self.policy_net(state).tolist())\n",
    "        return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RewardFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_func(transaction, close, is_none, is_done):\n",
    "    \n",
    "    if is_none:\n",
    "        ''' 取引をしていなければ0 '''\n",
    "        return -0.1\n",
    "\n",
    "    elif is_done:    \n",
    "        ''' 取引終了時にprofitが+であれば+1, -であれば-1 '''\n",
    "        profit = transaction.profit\n",
    "        return 1.0 if profit >= 0 else -1.0\n",
    "\n",
    "    else:\n",
    "        ''' 現在値がトレード時の値より高ければ0.1, 低ければ-0.1 '''\n",
    "        entry_rate = transaction.entry_rate # 取引中のトレードの始値\n",
    "        return 0.1 if close >= entry_rate else -0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EnvParameter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-864452dabcb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m''' 環境生成 '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mMONITOR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0menv_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_lot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMONITOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EnvParameter' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ''' 環境生成 '''\n",
    "    MONITOR = False\n",
    "    env_param = EnvParameter(max_lot=1, spread=1, window_size=30)\n",
    "    env = Environment(train_data, env_param, MONITOR, reward_func)\n",
    "    \n",
    "    ''' エージェント生成 '''\n",
    "    num_actions = env.get_action_num() \n",
    "    num_observ = env.get_observ_num()\n",
    "    brain_param = BrainParameter(batch_size=32, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=200, capacity=10000, hidden_size=100)\n",
    "    brain = Brain(brain_param, num_observ, num_actions)\n",
    "    agent = Agent(brain)\n",
    "    \n",
    "    ''' Trainer '''\n",
    "    cartpole_trainer = Trainer(env, agent)\n",
    "    cartpole_trainer.train('test.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ''' 環境生成 '''\n",
    "    MONITOR = False\n",
    "    env_param = EnvParameter(max_lot=1, spread=1, window_size=30)\n",
    "    env = Environment(None, valid_data, env_param, MONITOR)\n",
    "    \n",
    "    ''' エージェント生成 '''\n",
    "    num_actions = env.get_action_num() \n",
    "    num_observ = env.get_observ_num()\n",
    "    brain_param = BrainParameter(batch_size=32, gamma=0.99, eps_start=0.9, eps_end=0.05, eps_decay=200, capacity=10000, hidden_size=100)\n",
    "    brain = Brain(brain_param, num_observ, num_actions)\n",
    "    agent = Agent(brain)\n",
    "    \n",
    "    cartpole_examiner = Examiner(env, agent)\n",
    "    cartpole_examiner.evaluate('weight.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
